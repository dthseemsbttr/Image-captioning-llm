{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import razdel\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import telebot\n",
    "from telebot import types\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms \n",
    "import requests\n",
    "\n",
    "\n",
    "class Vectorizer:\n",
    "    pad = \"<PAD>\"\n",
    "    unk = \"<UNK>\"\n",
    "    sos = \"<SOS>\"\n",
    "    eos = \"<EOS>\"\n",
    "\n",
    "    def __init__(self, annotations):\n",
    "\n",
    "        words_with_dot_list = annotations.apply(\n",
    "            lambda x: self.tokenize(x))\n",
    "        words_with_dot = words_with_dot_list.explode()\n",
    "        words = words_with_dot.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "        self.counts = words.value_counts()\n",
    "        words = list(self.counts[self.counts > 2].index)\n",
    "        self.vocabulary = [Vectorizer.pad, Vectorizer.unk,\n",
    "                           Vectorizer.sos, Vectorizer.eos, *words]\n",
    "\n",
    "        text2seq = {word: i for i, word in enumerate(self.vocabulary)}\n",
    "        self.padding_idx = text2seq[Vectorizer.pad]\n",
    "        self.unknown_idx = text2seq[Vectorizer.unk]\n",
    "        self.start_of_sentance_idx = text2seq[Vectorizer.sos]\n",
    "        self.end_of_sentance_idx = text2seq[Vectorizer.eos]\n",
    "        self.text2seq = defaultdict(lambda: self.unknown_idx,  text2seq)\n",
    "        self.seq2text = {i: word for i, word in enumerate(self.vocabulary)}\n",
    "        max_len = max(words_with_dot_list.apply(lambda x: len(x)))\n",
    "        self.max_len = max_len + 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text_sub = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text_list = [_.text for _ in razdel.tokenize(text_sub.lower())]\n",
    "        return text_list\n",
    "\n",
    "    def encode(self, text):\n",
    "        no_pad = [self.start_of_sentance_idx] + list(map(lambda x: self.text2seq.get(\n",
    "            x, self.unknown_idx), self.tokenize(text))) + [self.end_of_sentance_idx]\n",
    "        len_pad = self.max_len - len(no_pad)\n",
    "        return torch.tensor(no_pad + [self.text2seq['<PAD>']]*len_pad)\n",
    "\n",
    "    def decode(self, encode_text):\n",
    "        with_pad = list(map(self.seq2text.get, encode_text.tolist(\n",
    "        ) if not isinstance(encode_text, list) else encode_text))\n",
    "        return ' '.join(list(filter(lambda x: x != '<PAD>', with_pad)))\n",
    "\n",
    "all_captions_path = 'D:/Вера/Диплом/all_captions.csv'\n",
    "vectorizer = Vectorizer(pd.read_csv(all_captions_path)['translations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.encoder.fc = nn.Linear(self.encoder.fc.in_features, 512)\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            nn.Linear(256*8, 128*14*14),\n",
    "            nn.Unflatten(1, (128, 14, 14)),\n",
    "\n",
    "            nn.Upsample(scale_factor=2.0, mode='nearest'),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2.0, mode='nearest'),\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2.0, mode='nearest'),\n",
    "\n",
    "            nn.Conv2d(32, 16, kernel_size=5, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2.0, mode='nearest'),\n",
    "\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(8, 3, 3, stride=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x) * 255\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        self.W = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.U = nn.Linear(encoder_dim, attention_dim)\n",
    "\n",
    "        self.A = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, features, hidden_state):\n",
    "\n",
    "        u_hs = self.U(features)\n",
    "        w_ah = self.W(hidden_state)\n",
    "\n",
    "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1))\n",
    "\n",
    "        attention_scores = self.A(combined_states)\n",
    "        attention_scores = attention_scores.squeeze(2)\n",
    "\n",
    "        alpha = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        attention_weights = features * alpha.unsqueeze(2)\n",
    "        attention_weights = attention_weights.sum(dim=1)\n",
    "\n",
    "        return alpha, attention_weights\n",
    "\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, embed_size: int, hidden_size: int, encoder_size: int, attention_dim: int, vocab: Vectorizer):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *list(Autoencoder().encoder.children())[:-2])\n",
    "\n",
    "        for param in list(self.encoder.parameters())[:-1]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.embedding = nn.Embedding(\n",
    "            len(self.vocab), embed_size, padding_idx=self.vocab.padding_idx)\n",
    "        self.pos_embeddings = nn.Embedding(self.vocab.max_len, embed_size)\n",
    "        self.attention = Attention(encoder_size, hidden_size, attention_dim)\n",
    "\n",
    "        self.init_h = nn.Linear(encoder_size, hidden_size)\n",
    "        self.init_c = nn.Linear(encoder_size, hidden_size)\n",
    "\n",
    "        self.lstm_cell = nn.LSTMCell(\n",
    "            embed_size+encoder_size, hidden_size, bias=True)\n",
    "        self.f_beta = nn.Linear(hidden_size, encoder_size)\n",
    "\n",
    "        self.fcn = nn.Linear(hidden_size, len(self.vocab))\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward_step(self, decoder_input, encoder_outputs, last_hidden, last_cell, position, device='cuda:0'):\n",
    "\n",
    "        embeds = self.embedding(\n",
    "            decoder_input) + self.pos_embeddings(torch.tensor(position).long().to(device))\n",
    "        alpha, context = self.attention(encoder_outputs, last_hidden)\n",
    "\n",
    "        lstm_input = torch.cat((embeds, context), dim=1)\n",
    "\n",
    "        hidden, cell = self.lstm_cell(lstm_input, (last_hidden, last_cell))\n",
    "\n",
    "        output = self.fcn(self.drop(hidden))\n",
    "\n",
    "        return output, alpha, hidden, cell\n",
    "\n",
    "    def forward(self, imgs, decoder_input, device='cuda:0'):\n",
    "\n",
    "        encoder_outputs = self.encoder(imgs)\n",
    "        encoder_outputs = encoder_outputs.permute(0, 2, 3, 1)\n",
    "        encoder_outputs = encoder_outputs.view(\n",
    "            encoder_outputs.size(0), -1, encoder_outputs.size(3))\n",
    "\n",
    "        hidden, cell = self.init_hidden_state(encoder_outputs)\n",
    "\n",
    "        seq_length = len(decoder_input[0])\n",
    "        batch_size = decoder_input.size(0)\n",
    "        num_features = encoder_outputs.size(1)\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_length,\n",
    "                              len(self.vocab)).to(device)\n",
    "        alphas = torch.zeros(batch_size, seq_length, num_features).to(device)\n",
    "\n",
    "        for s in range(seq_length):\n",
    "            output, alpha, hidden, cell = self.forward_step(\n",
    "                decoder_input[:, s].to(device), encoder_outputs, hidden, cell, s)\n",
    "            outputs[:, s] = output\n",
    "            alphas[:, s] = alpha\n",
    "\n",
    "        return outputs, alphas\n",
    "\n",
    "    def greedy_decode(self, imgs, device='cpu'):\n",
    "\n",
    "        encoder_outputs = self.encoder.to(device)(imgs)\n",
    "        encoder_outputs = encoder_outputs.permute(0, 2, 3, 1)\n",
    "        encoder_outputs = encoder_outputs.view(\n",
    "            encoder_outputs.size(0), -1, encoder_outputs.size(3))\n",
    "\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        hidden, cell = self.init_hidden_state(encoder_outputs)\n",
    "        decoder_input = torch.tensor(self.vocab.text2seq['<SOS>']).to(device)\n",
    "        decoder_input = torch.LongTensor([decoder_input]).to(device)\n",
    "\n",
    "        decoded_batch = [self.vocab.text2seq['<SOS>']]\n",
    "        for i in range(self.vocab.max_len):\n",
    "            decoder_output, alpha, hidden, cell = self.forward_step(\n",
    "                decoder_input, encoder_outputs, hidden, cell, i, 'cpu')\n",
    "\n",
    "            decoder_output = decoder_output.view(batch_size, -1)\n",
    "            predicted_word_idx = decoder_output.argmax(dim=1)\n",
    "            decoded_batch.append(predicted_word_idx.item())\n",
    "            if self.vocab.seq2text[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            decoder_input = predicted_word_idx\n",
    "        return decoded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionDecoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embedding): Embedding(18584, 512, padding_idx=0)\n",
       "  (pos_embeddings): Embedding(69, 512)\n",
       "  (attention): Attention(\n",
       "    (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (U): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (A): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (init_h): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (init_c): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (lstm_cell): LSTMCell(1024, 512)\n",
       "  (f_beta): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fcn): Linear(in_features=512, out_features=18584, bias=True)\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224))]) \n",
    "CAPTION_MODEL_weights = torch.load(\"./Caption Image/models/captioning_flickr_attention_pos_embedding_weights.pt\", map_location='cpu')\n",
    "\n",
    "\n",
    "embed_size=512\n",
    "attention_dim=512\n",
    "encoder_size=512\n",
    "hidden_size=512\n",
    "\n",
    "CAPTION_MODEL = AttentionDecoder(embed_size=embed_size, hidden_size=hidden_size, encoder_size=encoder_size, attention_dim=attention_dim, vocab=vectorizer)\n",
    "CAPTION_MODEL.load_state_dict(CAPTION_MODEL_weights)\n",
    "CAPTION_MODEL.encoder.eval()\n",
    "CAPTION_MODEL.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': {'alternatives': [{'message': {'role': 'assistant', 'text': 'На фото изображена прыгающая собака.'}, 'status': 'ALTERNATIVE_STATUS_FINAL'}], 'usage': {'inputTextTokens': '84', 'completionTokens': '9', 'totalTokens': '93'}, 'modelVersion': '07.03.2024'}}\n"
     ]
    }
   ],
   "source": [
    "bot = telebot.TeleBot('7090775661:AAHIZsPF22Dn2_9_5ls_iIFUnNO0jq0gc28')\n",
    "\n",
    "@bot.message_handler(commands=['start'])\n",
    "def start(message):\n",
    "    bot.send_message(message.chat.id, 'Привет! Я - модель глубокого обучения, созданная для описания изображений')\n",
    "    markup_inline = types.InlineKeyboardMarkup()\n",
    "    caption = types.InlineKeyboardButton(text = 'Описание фотографии', callback_data = 'caption')\n",
    "    llm_y = types.InlineKeyboardButton(text = 'Спросить у Yandex-GPT', callback_data = 'llm_yagpt')\n",
    "    llm_g = types.InlineKeyboardButton(text = 'Спросить у GigaChat', callback_data = 'llm_giga')\n",
    "    markup_inline.add(caption, llm_y, llm_g)\n",
    "    bot.send_message(message.chat.id, 'Что Вы хотите сделать?', reply_markup=markup_inline)\n",
    "\n",
    "@bot.callback_query_handler(func=lambda call: 'caption' in call.data)   \n",
    "def call_caption(call):\n",
    "    action = 'описание'\n",
    "    msg = bot.send_message(call.from_user.id, 'Пришлите фото')\n",
    "    tensor = bot.register_next_step_handler(msg, partial(get_photo, call, action, None))\n",
    "    \n",
    "@bot.callback_query_handler(func=lambda call: 'llm_giga' in call.data)   \n",
    "def call_caption(call):\n",
    "    action = 'вопрос'\n",
    "    msg = bot.send_message(call.from_user.id, 'Пришлите фото')\n",
    "    bot.register_next_step_handler(msg, partial(get_photo, call, action, ask_llm_giga))\n",
    "\n",
    "@bot.callback_query_handler(func=lambda call: 'llm_yagpt' in call.data)   \n",
    "def call_caption(call):\n",
    "    action = 'вопрос'\n",
    "    msg = bot.send_message(call.from_user.id, 'Пришлите фото')\n",
    "    bot.register_next_step_handler(msg, partial(get_photo, call, action, ask_llm_yagpt))\n",
    "    \n",
    "def get_photo(call, action, handler, message):\n",
    "    fileID = message.photo[-1].file_id   \n",
    "    file_info = bot.get_file(fileID)\n",
    "    downloaded_file = bot.download_file(file_info.file_path)\n",
    "    with open(\"image.jpg\", 'wb') as new_file:\n",
    "        new_file.write(downloaded_file)\n",
    "    image = torchvision.io.read_image('image.jpg') \n",
    "    img_tensor = transform(image)\n",
    "    if action == 'описание':\n",
    "        make_caption(call, img_tensor)\n",
    "    elif action == 'вопрос':\n",
    "        msg = bot.send_message(call.from_user.id, 'Какой вопрос вы хотите задать?')\n",
    "        bot.register_next_step_handler(msg, partial(handler, call, img_tensor))\n",
    "\n",
    "def make_caption(call, img_tensor):\n",
    "    caption_ready = CAPTION_MODEL.greedy_decode(img_tensor.float().unsqueeze(0))\n",
    "    bot.send_message(call.from_user.id, vectorizer.decode(caption_ready[1:-1]))\n",
    "    start(call.message)\n",
    "\n",
    "def ask_llm_yagpt(call, img_tensor, message):\n",
    "    question = message.text\n",
    "    caption_ready = CAPTION_MODEL.greedy_decode(img_tensor.float().unsqueeze(0))\n",
    "    ask_to_llm_yagpt(call, vectorizer.decode(caption_ready[1:-1]), question)\n",
    "\n",
    "def ask_llm_giga(call, img_tensor, message):\n",
    "    question = message.text\n",
    "    caption_ready = CAPTION_MODEL.greedy_decode(img_tensor.float().unsqueeze(0))\n",
    "    ask_to_llm_giga(call, vectorizer.decode(caption_ready[1:-1]), question)\n",
    "\n",
    "def ask_to_llm_giga(call, caption, question):\n",
    "    url = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n",
    "    payload='scope=GIGACHAT_API_PERS'\n",
    "    headers = {\n",
    "      'Content-Type': 'application/x-www-form-urlencoded',\n",
    "      'Accept': 'application/json',\n",
    "      'RqUID': '39acb689-2cd5-4190-be44-310b49698dc7',\n",
    "      'Authorization': 'Basic MjU5YWY5NzctNGY4MC00MTk0LThiNjktZThjMzVlNDExMTQ0OjM5YWNiNjg5LTJjZDUtNDE5MC1iZTQ0LTMxMGI0OTY5OGRjNw=='\n",
    "    }\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload).json()\n",
    "    auth = response['access_token']\n",
    "    \n",
    "    url = \"https://gigachat.devices.sberbank.ru/api/v1/chat/completions\"\n",
    "    payload = json.dumps({\n",
    "      \"model\": \"GigaChat\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"Тебе на вход поступит вопрос и описание картинки, полученное с помощью другой нейросети Image Captioning. Постарайся максимально точно ответить на вопрос о картинке исходя из её описания. Не обязательно использовать все распознанные объекты на изображении для формирования ответа.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"{question}\\nОписание картинки: {caption}\"\n",
    "        }\n",
    "      ],\n",
    "      \"temperature\": 0.15,\n",
    "      \"top_p\": 0.1,\n",
    "      \"n\": 1,\n",
    "      \"stream\": False,\n",
    "      \"max_tokens\": 300,\n",
    "      \"repetition_penalty\": 1\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'Accept': 'application/json',\n",
    "      'Authorization': f'Bearer {auth}'\n",
    "    }\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload).json()\n",
    "    return responce['choices'][0]['message']['content']\n",
    "    \n",
    "def ask_to_llm_yagpt(call, caption, question):\n",
    "    api_key = 'AQVN1wuefTqX_WAQa6YAvx4mduQtzaX1OAXOcso2'\n",
    "\n",
    "    url = \"https://llm.api.cloud.yandex.net/foundationModels/v1/completion\"\n",
    "\n",
    "    headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": \"Api-Key {0}\".format(api_key)\n",
    "    }\n",
    "\n",
    "    json_body = {\n",
    "      \"modelUri\": \"gpt://b1gnc3j9s2j025i5f63o/yandexgpt/latest\",\n",
    "      \"completionOptions\": {\n",
    "        \"stream\": False,\n",
    "        \"temperature\": 0.15,\n",
    "        \"maxTokens\": 300\n",
    "      },\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"text\": \"Тебе на вход поступит вопрос и описание картинки, полученное с помощью другой нейросети Image Captioning. Постарайся максимально точно ответить на вопрос о картинке исходя из её описания. Не обязательно использовать все распознанные объекты на изображении для формирования ответа.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"text\": f\"{question}\\nОписание картинки: {caption}\"\n",
    "        }\n",
    "      ]\n",
    "    }   \n",
    "    responce = requests.post(url, headers=headers, json=json_body)\n",
    "    print(responce.json())\n",
    "    bot.send_message(call.from_user.id, responce.json()['result']['alternatives'][0]['message']['text'])\n",
    "    start(call.message)\n",
    "    \n",
    "bot.polling(none_stop=True, interval=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
